File Name: __init__.py
========================================

========================================

File Name: agents\__init__.py
========================================

========================================

File Name: agents\langgraph\__init__.py
========================================

========================================

File Name: agents\langgraph\complex_agent\agent.py
========================================
from .graph import rag_pipeline
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

class ComplexRAGAgent:
    def __init__(self):
        self.pipeline = rag_pipeline

    def run(self, question: str):
        inputs = {"question": question}
        result = self.pipeline.invoke(inputs)
        return result["generation"]

    def stream(self, question: str):
        inputs = {"question": question}
        for output in self.pipeline.stream(inputs, stream_mode='updates'):
            yield output
               
========================================

File Name: agents\langgraph\complex_agent\chains.py
========================================
from typing import Literal
import os
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from .prompts import(
    rag_prompt,
    db_query_rewrite_prompt,
    hallucination_prompt,
    answer_prompt,
    query_feedback_prompt,
    generation_feedback_prompt,
    give_up_prompt,
    grade_doc_prompt,
    knowledge_extraction_prompt,
    router_prompt,
    websearch_query_rewrite_prompt,
    simple_question_prompt
)
from dotenv import load_dotenv

load_dotenv()

class GradeHallucinations(BaseModel):
    binary_score: Literal["yes", "no"] = Field(
        description="Answer is grounded in the facts, 'yes' or 'no'"
    )

class GradeDocuments(BaseModel):
    binary_score: Literal["yes", "no"] = Field(
        description="Document is relevant to the question, 'yes' or 'no'"
    )

class GradeAnswer(BaseModel):
    binary_score: Literal["yes", "no"] = Field(
        description="Answer addresses the question, 'yes' or 'no'"
    )

class RouteQuery(BaseModel):
    route: Literal["vectorstore", "websearch", "QA_LM"] = Field(
        description="Given a user question choose to route it to web search (websearch), a vectorstore (vectorstore), or a QA language model (QA_LM).",
    )

llm_engine = ChatOpenAI(model='gpt-4o-mini')

rag_chain = rag_prompt | llm_engine | StrOutputParser()
db_query_rewriter = db_query_rewrite_prompt | llm_engine | StrOutputParser()
hallucination_grader = hallucination_prompt | llm_engine.with_structured_output(GradeHallucinations)
answer_grader = answer_prompt | llm_engine.with_structured_output(GradeAnswer)
query_feedback_chain = query_feedback_prompt | llm_engine | StrOutputParser()
generation_feedback_chain = generation_feedback_prompt | llm_engine | StrOutputParser()
give_up_chain = give_up_prompt | llm_engine | StrOutputParser()
retrieval_grader = grade_doc_prompt | llm_engine.with_structured_output(GradeDocuments)
knowledge_extractor = knowledge_extraction_prompt | llm_engine | StrOutputParser()
question_router = router_prompt | llm_engine.with_structured_output(RouteQuery)
websearch_query_rewriter = websearch_query_rewrite_prompt | llm_engine | StrOutputParser()
simple_question_chain = simple_question_prompt | llm_engine | StrOutputParser()
========================================

File Name: agents\langgraph\complex_agent\graph.py
========================================
from langgraph.graph import END, StateGraph, START
from .nodes import (
    GraphState,
    db_query_rewriting_node,
    retriever_node,
    generation_node,
    router_node,
    query_feedback_node,
    generation_feedback_node,
    simple_question_node,
    web_search_node,
    websearch_query_rewriting_node,
    give_up_node,
    filter_relevant_documents_node,
    knowledge_extractor_node,
    answer_evaluation_node,
    search_mode_node,
    relevant_documents_validation_node
)


pipeline = StateGraph(GraphState)

pipeline.add_node('db_query_rewrite_node', db_query_rewriting_node)
pipeline.add_node('retrieval_node', retriever_node)
pipeline.add_node('generator_node', generation_node)
pipeline.add_node('query_feedback_node', query_feedback_node)
pipeline.add_node('generation_feedback_node', generation_feedback_node)
pipeline.add_node('simple_question_node', simple_question_node)
pipeline.add_node('websearch_query_rewriting_node', websearch_query_rewriting_node)
pipeline.add_node('web_search_node', web_search_node)
pipeline.add_node('give_up_node', give_up_node)
pipeline.add_node('filter_docs_node', filter_relevant_documents_node)
pipeline.add_node('extract_knowledge_node', knowledge_extractor_node)

pipeline.add_conditional_edges(
    START, 
    router_node,
    {
        "vectorstore": 'db_query_rewrite_node',
        "websearch": 'websearch_query_rewriting_node',
        "QA_LM": 'simple_question_node'
    },
)

pipeline.add_edge('db_query_rewrite_node', 'retrieval_node')
pipeline.add_edge('retrieval_node', 'filter_docs_node')
pipeline.add_edge('extract_knowledge_node', 'generator_node')
pipeline.add_edge('websearch_query_rewriting_node', 'web_search_node')
pipeline.add_edge('web_search_node', 'filter_docs_node')
pipeline.add_edge('generation_feedback_node', 'generator_node')
pipeline.add_edge('simple_question_node', END)
pipeline.add_edge('give_up_node', END)

pipeline.add_conditional_edges(
    'generator_node', 
    answer_evaluation_node,
    {
        "useful": END,
        "not relevant": 'query_feedback_node',
        "hallucination": 'generation_feedback_node',
        "max_generation_reached": 'give_up_node'
    }  
)

pipeline.add_conditional_edges(
    'query_feedback_node', 
    search_mode_node,
    {
        "vectorstore": 'db_query_rewrite_node',
        "websearch": 'websearch_query_rewriting_node',
    }
)

pipeline.add_conditional_edges(
    'filter_docs_node', 
    relevant_documents_validation_node,
    {
        "knowledge_extraction": 'extract_knowledge_node',
        "websearch": 'websearch_query_rewriting_node',
        "vectorstore": 'db_query_rewrite_node',
        "max_db_search": 'websearch_query_rewriting_node',
        "max_websearch": 'give_up_node'
    }
)

rag_pipeline = pipeline.compile()
========================================

File Name: agents\langgraph\complex_agent\nodes.py
========================================
from app.core.indexers.chroma_indexer import ChromaIndexer
from .chains import (
    rag_chain,
    db_query_rewriter,
    hallucination_grader,
    answer_grader,
    generation_feedback_chain,
    query_feedback_chain,
    retrieval_grader,
    knowledge_extractor,
    question_router,
    simple_question_chain,
    give_up_chain,
    websearch_query_rewriter
)
from .tools import web_search_tool
from .state import GraphState
import logging

logger = logging.getLogger(__name__)

indexer = ChromaIndexer("default_collection")
retriever = indexer.as_retriever()

MAX_RETRIEVALS = 3
MAX_GENERATIONS = 3


def retriever_node(state: GraphState):
    new_documents = retriever.invoke(state.rewritten_question)
    new_documents = [d.page_content for d in new_documents]
    state.documents.extend(new_documents)
    return {
        "documents": state.documents, 
        "retrieval_num": state.retrieval_num + 1
    }

def generation_node(state: GraphState):
    generation = rag_chain.invoke({
        "context": "\n\n".join(state.documents), 
        "question": state.question, 
        "feedback": "\n".join(state.generation_feedbacks)
    })
    return {
        "generation": generation,
        "generation_num": state.generation_num + 1
    }

def db_query_rewriting_node(state: GraphState):
    rewritten_question = db_query_rewriter.invoke({
        "question": state.question,
        "feedback": "\n".join(state.query_feedbacks)
    })
    return {"rewritten_question": rewritten_question, "search_mode": "vectorstore"} 

def answer_evaluation_node(state: GraphState):
    # assess hallucination
    hallucination_grade = hallucination_grader.invoke(
        {"documents": state.documents, "generation": state.generation}
    )
    if hallucination_grade.binary_score == "yes":
        # if no hallucination, assess relevance
        answer_grade = answer_grader.invoke({
            "question": state.question, 
            "generation": state.generation
        })
        if answer_grade.binary_score == "yes":
            # no hallucination and relevant
            return "useful"
        elif state.generation_num > MAX_GENERATIONS:
            return "max_generation_reached"
        else:
            # no hallucination but not relevant
            return "not relevant"
    elif state.generation_num > MAX_GENERATIONS:
        return "max_generation_reached"
    else:
        # we have hallucination
        return "hallucination" 
    
def generation_feedback_node(state: GraphState):
    feedback = generation_feedback_chain.invoke({
        "question": state.question,
        "documents": "\n\n".join(state.documents),
        "generation": state.generation
    })

    feedback = 'Feedback about the answer "{}": {}'.format(
        state.generation, feedback
    )
    state.generation_feedbacks.append(feedback)
    return {"generation_feedbacks": state.generation_feedbacks}

def query_feedback_node(state: GraphState):
    feedback = query_feedback_chain.invoke({
        "question": state.question,
        "rewritten_question": state.rewritten_question,
        "documents": "\n\n".join(state.documents),
        "generation": state.generation
    })

    feedback = 'Feedback about the query "{}": {}'.format(
        state.rewritten_question, feedback
    )
    state.query_feedbacks.append(feedback)
    return {"query_feedbacks": state.query_feedbacks}

def give_up_node(state: GraphState):
    response = give_up_chain.invoke(state.question)
    return {"generation": response}

def filter_relevant_documents_node(state: GraphState):
    # first, we grade every documents
    grades = retrieval_grader.batch([
        {"question": state.question, "document": doc} 
        for doc in state.documents
    ])
    # Then we keep only the documents that were graded as relevant
    filtered_docs = [
        doc for grade, doc 
        in zip(grades, state.documents) 
        if grade.binary_score == 'yes'
    ]

    # If we didn't get any relevant document, let's capture that 
    # as a feedback for the next retrieval iteration
    if not filtered_docs:
        feedback = 'Feedback about the query "{}": did not generate any relevant documents.'.format(
            state.rewritten_question
        )
        state.query_feedbacks.append(feedback)

    return {
        "documents": filtered_docs, 
        "query_feedbacks": state.query_feedbacks
    }

def knowledge_extractor_node(state: GraphState):
    filtered_docs = knowledge_extractor.batch([
        {"question": state.question, "document": doc} 
        for doc in state.documents
    ])
    # we keep only the non empty documents
    filtered_docs = [doc for doc in filtered_docs if doc]
    return {"documents": filtered_docs}

def router_node(state: GraphState):
    route_query = question_router.invoke(state.question)
    return route_query.route

def simple_question_node(state: GraphState):
    answer = simple_question_chain.invoke(state.question)
    return {"generation": answer, "search_mode": "QA_LM"}

def websearch_query_rewriting_node(state: GraphState):
    rewritten_question = websearch_query_rewriter.invoke({
        "question": state.question, 
        "feedback": "\n".join(state.query_feedbacks)
    })
    if state.search_mode != "websearch":
        state.retrieval_num = 0    
    return {
        "rewritten_question": rewritten_question, 
        "search_mode": "websearch",
        "retrieval_num": state.retrieval_num
    }

def web_search_node(state: GraphState):
    try:
        new_docs = web_search_tool.invoke(
            {"query": state.rewritten_question}
        )
        
        if isinstance(new_docs, str):
            web_results = [new_docs]
        elif isinstance(new_docs, list):
            web_results = [d.get("content", str(d)) if isinstance(d, dict) else str(d) for d in new_docs]
        else:
            web_results = [str(new_docs)]
        
        state.documents.extend(web_results)
        return {
            "documents": state.documents, 
            "retrieval_num": state.retrieval_num + 1
        }
    except Exception as e:
        return {
            "error": f"Web search failed: {str(e)}",
            "retrieval_num": state.retrieval_num + 1
        }

def search_mode_node(state: GraphState):
    return state.search_mode

def relevant_documents_validation_node(state: GraphState):
    if state.documents:
        # we have relevant documents
        return "knowledge_extraction"
    elif state.search_mode == 'vectorsearch' and state.retrieval_num > MAX_RETRIEVALS:
        # we don't have relevant documents
        # and we reached the maximum number of retrievals
        return "max_db_search"
    elif state.search_mode == 'websearch' and state.retrieval_num > MAX_RETRIEVALS:
        # we don't have relevant documents
        # and we reached the maximum number of websearches
        return "max_websearch"
    else:
        # we don't have relevant documents
        # so we retry the search
        return state.search_mode
========================================

File Name: agents\langgraph\complex_agent\prompts.py
========================================
from langchain_core.prompts import ChatPromptTemplate


system_prompt = """
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Additional feedback may be provided about a previous version of the answer. Make sure to utilize that feedback to improve the answer.
Only provide the answer and nothing else!
"""

human_prompt = """
Question: {question}

Context: 
{context}

Here is the feedback about previous versions of the answer:
{feedback}

Answer:
"""

rag_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

#***#
system_prompt = """
You a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval.
The vectorstore contains information about AI papers. Look at the input and try to reason about the underlying semantic intent / meaning.
Additional feedback may be provided for why a previous version of the question didn't lead to a valid response. Make sure to utilize that feedback to generate a better question.
Only respond with the rewritten question and nothing else! 
"""

human_prompt = """
Here is the initial question: {question}

Here is the feedback about previous versions of the question:
{feedback}

Formulate an improved question.
Rewritten question:
"""

db_query_rewrite_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.
Give a binary score 'yes' or 'no'. 'yes' means that the answer is grounded in / supported by the set of facts.
"""

human_prompt = """
Set of facts:

{documents}

LLM generation: {generation}
"""

hallucination_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
) 

system_prompt = """
You are a grader assessing whether an answer addresses / resolves a question.
Give a binary score 'yes' or 'no'. 'yes' means that the answer resolves the question.
"""

human_prompt = """
User question: {question} 

LLM generation: {generation}
"""


answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
Your role is to give feedback on a the LLM generated answer. The LLM generation is NOT grounded in the set of retrieved facts.
Explain how the generated answer could be improved so that it is only solely grounded in the retrieved facts.  
Only provide your feedback and nothing else!
"""

human_prompt = """
User question: {question}

Retrieved facts: 
{documents}

Wrong generated answer: {generation}
"""

generation_feedback_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
Your role is to give feedback on a the text query used to retrieve documents. Those retrieved documents are used as context to answer a user question.
The following generated answer doesn't address the question! Explain how the query could be improved so that the retrieved documents could be more relevant to the question. 
Only provide your feedback and nothing else!
"""

human_prompt = """
User question: {question}

Text query: {rewritten_question}

Retrieved documents: 
{documents}

Wrong generated answer: {generation}
"""

query_feedback_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
You job is to generate an apology for not being able to provide a correct answer to a user question.
The question were used to retrieve documents from a database and a websearch and none of them were able to provide enough context to answer the user question.
Explain to the user that you couldn't answer the question.
"""

give_up_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "User question: {question} \n\n Answer:"),
    ]
)

system_prompt = """
You are a grader assessing relevance of a retrieved document to a user question. 
It does not need to be a stringent test. The goal is to filter out erroneous retrievals.
If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.
Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. 'yes' means that the document contains relevant information.
"""

human_prompt = """
Retrieved document: {document}

User question: {question}
"""

grade_doc_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
You are a knowledge refinement engine. Your job is to extract the information from a document that could be relevant to a user question. 
The goal is to filter out the noise and keep only the information that can provide context to answer the user question.
If the document contains keyword(s) or semantic meaning related to the user question, consider it as relevant.
DO NOT modify the text, only return the original text that is relevant to the user question. 
"""

human_prompt = """
Retrieved document: {document}

User question: {question}
"""

knowledge_extraction_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

#***#
system_prompt = """
You are an expert at routing a user question to a vectorstore, a websearch or a simple QA language model.
The vectorstore contains documents related to AI papers.
If you can answer the question without any additional context or if a websearch could not provide additional context, route it to the QA language model.
If you need additional context and it is a question about AI papers, use the vectorstore, otherwise, use websearch.
"""

router_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{question}"),
    ]
)

system_prompt = """
You are a question re-writer that converts an input question to a better version that is optimized for web search. 
Look at the input and try to reason about the underlying semantic intent / meaning.
Additional feedback may be provided for why a previous version of the question didn't lead to a valid response. Make sure to utilize that feedback to generate a better question.
Only respond with the rewritten question and nothing else! 
"""

human_prompt = """
Here is the initial question: {question}

Here is the feedback about previous versions of the question:
{feedback}

Formulate an improved question.
Rewritten question:
"""

websearch_query_rewrite_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
You are a helpful assistant. Provide a answer to the user.
"""

simple_question_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{question}"),
    ]
)
========================================

File Name: agents\langgraph\complex_agent\state.py
========================================
from pydantic import BaseModel
from typing import List, Literal, Optional

class GraphState(BaseModel):

    question: Optional[str] = None
    generation: Optional[str] = None
    documents: List[str] = []
    rewritten_question: Optional[str] = None
    query_feedbacks: List[str] = []
    generation_feedbacks: List[str] = []
    generation_num: int = 0
    retrieval_num: int = 0
    search_mode: Literal["vectorstore", "websearch", "QA_LM"] = "QA_LM"
========================================

File Name: agents\langgraph\complex_agent\tools.py
========================================
import os
from langchain_community.tools.tavily_search import TavilySearchResults
from dotenv import load_dotenv

load_dotenv()

web_search_tool = TavilySearchResults(k=3)
========================================

File Name: agents\langgraph\complex_agent\__init__.py
========================================

========================================

File Name: agents\langgraph\simple_agent\agent.py
========================================
from langchain_core.retrievers import BaseRetriever
from langchain_core.runnables import RunnableSequence
from .graph import build_rag_pipeline
from .state import GraphState

class SimpleRAGAgent:
    def __init__(self, retriever: BaseRetriever, generation_chain: RunnableSequence):
        self.retriever = retriever
        self.generation_chain = generation_chain
        self.pipeline = build_rag_pipeline(self.retriever, self.generation_chain)

    def run(self, question: str):
        inputs = {"question": question}
        result = self.pipeline.invoke(inputs)
        return result["generation"]

    def stream(self, question: str):
        inputs = {"question": question}
        for output in self.pipeline.stream(inputs, stream_mode='updates'):
            yield output
               
            
========================================

File Name: agents\langgraph\simple_agent\chains.py
========================================
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from .prompts import rag_prompt

load_dotenv()

llm_engine = ChatOpenAI(model='gpt-4o-mini')  
rag_chain = rag_prompt | llm_engine | StrOutputParser()
========================================

File Name: agents\langgraph\simple_agent\graph.py
========================================
from langgraph.graph import StateGraph, START, END
from langchain_core.retrievers import BaseRetriever
from langchain_core.runnables import RunnableSequence
from .state import GraphState
from .nodes import create_retriever_node, create_generation_node

def build_rag_pipeline(retriever: BaseRetriever, generation_chain: RunnableSequence):
    pipeline = StateGraph(GraphState)
    
    # Create nodes
    retriever_node = create_retriever_node(retriever)
    generator_node = create_generation_node(generation_chain)
    
    # Add nodes
    pipeline.add_node('retrieval_node', retriever_node)
    pipeline.add_node('generator_node', generator_node)
    
    # Connect nodes
    pipeline.add_edge(START, 'retrieval_node')
    pipeline.add_edge('retrieval_node', 'generator_node')
    pipeline.add_edge('generator_node', END)
    
    return pipeline.compile()
========================================

File Name: agents\langgraph\simple_agent\nodes.py
========================================
from langchain_core.retrievers import BaseRetriever
from langchain_core.runnables import RunnableSequence
from app.core.indexers.chroma_indexer import ChromaIndexer
from .chains import rag_chain
from .state import GraphState

def create_retriever_node(retriever: BaseRetriever):
    def retriever_node(state: GraphState):
        new_documents = retriever.invoke(state.question)
        new_documents = [d.page_content for d in new_documents]
        state.documents.extend(new_documents)
        return {"documents": state.documents}
    return retriever_node

def create_generation_node(generation_chain: RunnableSequence):
    def generation_node(state: GraphState):
        generation = generation_chain.invoke({
            "context": "\n\n".join(state.documents), 
            "question": state.question, 
        })
        return {"generation": generation}
    return generation_node
========================================

File Name: agents\langgraph\simple_agent\prompts.py
========================================
from langchain_core.prompts import ChatPromptTemplate

system_prompt = """
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Only provide the answer and nothing else!
"""

human_prompt = """
Question: {question}

Context: 
{context}

Answer:
"""

rag_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)


========================================

File Name: agents\langgraph\simple_agent\state.py
========================================
from typing import List, Optional
from pydantic import BaseModel

class GraphState(BaseModel):
    question: Optional[str] = None
    generation: Optional[str] = None
    documents: List[str] = []
========================================

File Name: agents\langgraph\simple_agent\__init__.py
========================================
from .agent import SimpleRAGAgent
from app.core.indexers.chroma_indexer import ChromaIndexer
from .chains import rag_chain

def create_chroma_simple_rag_agent(collection_name: str = "default_collection"):
    indexer = ChromaIndexer(collection_name)
    retriever = indexer.as_retriever()
    return SimpleRAGAgent(retriever, rag_chain)
========================================

File Name: chunkers\simple_chunker.py
========================================
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

class SimpleChunker:
    def __init__(self, chunk_size=10000, chunk_overlap=200):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        return self.text_splitter.split_documents(documents)
    
    def split_text(self, text: str) -> List[str]:
        return self.text_splitter.split_text(text)
========================================

File Name: chunkers\__init__.py
========================================

========================================

File Name: indexers\chroma_indexer.py
========================================
from typing import List
from langchain_core.documents import Document
from app.databases.chroma_db import chroma_db

class ChromaIndexer:
    def __init__(self, collection_name="default_collection"):
        self.collection_name = collection_name
        self.vectorstore = chroma_db.initialize_db(collection_name)
    
    def add_documents(self, documents: List[Document]):
        self.vectorstore.add_documents(documents)
        #self.vectorstore.persist()
    
    def similarity_search(self, query: str, k: int = 4):
        return self.vectorstore.similarity_search(query, k=k)
    
    def update_document(self, document_id: str, document: Document):
        self.vectorstore.update_document(document_id, document)
        #self.vectorstore.persist()
    
    def delete_document(self, document_id: str):
        self.vectorstore.delete([document_id])
        #self.vectorstore.persist()
    
    def as_retriever(self, search_kwargs=None):
        if search_kwargs is None:
            search_kwargs = {"k": 4}
        return self.vectorstore.as_retriever(search_kwargs=search_kwargs)
    
    def get_collection(self):
        return chroma_db.load_db(self.collection_name)
    
    def count_documents(self):
        return self.vectorstore._collection.count()
========================================

File Name: indexers\__init__.py
========================================

========================================

File Name: loaders\gdrive_loader.py
========================================
import os
from app.services import google_drive
import logging

logger = logging.getLogger(__name__)

class GDriveLoader:
    def __init__(self):
        self.service = None

    def authenticate(self):
        return google_drive.authenticate()

    def set_credentials(self, authorization_response, state):
        creds = google_drive.get_credentials_from_callback(authorization_response, state)
        google_drive.save_credentials(creds)

    def initialize_service(self):
        if not self.service:
            self.service = google_drive.get_service()

    def download_files(self, folder_id):
        """Download files from a Google Drive folder"""
        try:
            if not self.service:
                self.initialize_service()

            downloaded_files = google_drive.download_files(self.service, folder_id)
            
            if not downloaded_files:
                logger.warning(f"No files found in folder {folder_id}")
                return []

            # Return only the filenames for the UI
            return [os.path.basename(file_path) for file_path in downloaded_files]

        except Exception as e:
            logger.error(f"Error downloading files: {str(e)}")
            raise
========================================

File Name: loaders\__init__.py
========================================

========================================

File Name: pipes\simple_index_pipeline.py
========================================
import os
from typing import List
from langchain_community.document_loaders import PyPDFLoader
from app.core.chunkers.simple_chunker import SimpleChunker
from app.core.indexers.chroma_indexer import ChromaIndexer
from langchain_core.documents import Document

class SimpleIndexChromaPipeline:
    def __init__(self, collection_name: str, chunk_size: int = 10000, chunk_overlap: int = 200):
        self.collection_name = collection_name
        self.loader = PyPDFLoader
        self.chunker = SimpleChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        self.indexer = ChromaIndexer(collection_name=collection_name)

    def process_pdf(self, file_path: str) -> List[Document]:
        # Load PDF
        loader = self.loader(file_path)
        documents = loader.load()
        
        # Add file metadata to each document
        filename = os.path.basename(file_path)
        for doc in documents:
            doc.metadata.update({
                "source_file": filename,
                "file_path": file_path,
                "page_number": doc.metadata.get("page", 1)
            })

        # Chunk documents
        chunked_documents = self.chunker.split_documents(documents)
        
        # Ensure metadata is preserved in chunks
        for chunk in chunked_documents:
            if not chunk.metadata.get("source_file"):
                chunk.metadata.update({
                    "source_file": filename,
                    "file_path": file_path,
                    "page_number": chunk.metadata.get("page", 1)
                })

        # Index documents
        self.indexer.add_documents(chunked_documents)

        return chunked_documents

    def process_multiple_pdfs(self, file_paths: List[str]) -> List[Document]:
        all_documents = []
        for file_path in file_paths:
            all_documents.extend(self.process_pdf(file_path))
        return all_documents

    def process_folder(self, folder_path: str) -> List[Document]:
        all_documents = []
        for root, _, files in os.walk(folder_path):
            for file in files:
                if file.lower().endswith('.pdf'):
                    file_path = os.path.join(root, file)
                    all_documents.extend(self.process_pdf(file_path))
        return all_documents
========================================

File Name: pipes\__init__.py
========================================

========================================

File Name: processors\__init__.py
========================================

========================================

