File Name: api\app.py
========================================
from fastapi import FastAPI
from app.api.routers import download_router, chromadb_router, chromaindexer_router, chromaagent_router
from dotenv import load_dotenv

load_dotenv()

app = FastAPI()

app.include_router(download_router.router)
app.include_router(chromadb_router.router)
app.include_router(chromaindexer_router.router)
app.include_router(chromaagent_router.router)

@app.get("/")
def root():
    return {"message": "Welcome to AIIP AI Agents"}
========================================

File Name: api\__init__.py
========================================

========================================

File Name: api\routers\chromaagent_router.py
========================================
from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import StreamingResponse
from app.core.agents.langgraph.simple_agent import create_chroma_simple_rag_agent
from app.core.agents.langgraph.complex_agent.agent import ComplexRAGAgent
import json
import logging

router = APIRouter(prefix="/agent", tags=["Agent"])

@router.post("/simple_rag/{collection_name}/query")
async def query_simple_rag_agent(
    collection_name: str,
    question: str = Query(..., description="The question to ask the RAG agent")
):
    try:
        agent = create_chroma_simple_rag_agent(collection_name)
        result = agent.run(question)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/simple_rag/{collection_name}/stream")
async def stream_simple_rag_agent(
    collection_name: str,
    question: str = Query(..., description="The question to ask the RAG agent")
):
    try:
        agent = create_chroma_simple_rag_agent(collection_name)
        
        def event_generator():
            for output in agent.stream(question):
                yield f"data: {json.dumps(output)}\n\n"

        return StreamingResponse(event_generator(), media_type="text/event-stream")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/complex_rag/query")
async def query_complex_rag_agent(
    question: str = Query(..., description="The question to ask the complex RAG agent")
):
    try:
        agent = ComplexRAGAgent()
        result = agent.run(question)
        if result.startswith("Error:"):
            raise HTTPException(status_code=500, detail=result)
        return {"answer": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/complex_rag/stream")
async def stream_complex_rag_agent(
    question: str = Query(..., description="The question to ask the complex RAG agent")
):
    try:
        agent = ComplexRAGAgent()
        
        def event_generator():
            for output in agent.stream(question):
                if isinstance(output, dict) and "error" in output:
                    yield f"data: {json.dumps({'error': output['error']})}\n\n"
                    break
                yield f"data: {json.dumps(output)}\n\n"

        return StreamingResponse(event_generator(), media_type="text/event-stream")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
========================================

File Name: api\routers\chromadb_router.py
========================================
from fastapi import APIRouter, HTTPException
from app.databases.chroma_db import chroma_db
import logging

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/chromadb", tags=["Databases"])

@router.post("/create")
async def create_database():
    try:
        chroma_db.initialize_db()
        return {"message": "Database created successfully"}
    except Exception as e:
        logger.error(f"Error creating database: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/collections/{collection_name}")
async def create_collection(collection_name: str):
    try:
        chroma_db.create_collection(collection_name)
        return {"message": f"Collection '{collection_name}' created successfully"}
    except Exception as e:
        logger.error(f"Error creating collection: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/collections/{collection_name}")
async def delete_collection(collection_name: str):
    try:
        chroma_db.delete_collection(collection_name)
        return {"message": f"Collection '{collection_name}' deleted successfully"}
    except Exception as e:
        logger.error(f"Error deleting collection: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/collections")
async def list_collections():
    try:
        collections = chroma_db.list_collections()
        return {"collections": collections}
    except Exception as e:
        logger.error(f"Error listing collections: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
========================================

File Name: api\routers\chromaindexer_router.py
========================================
from fastapi import APIRouter, HTTPException, Body, File, UploadFile
from typing import List
from app.core.indexers.chroma_indexer import ChromaIndexer
from app.core.pipes.simple_index_pipeline import SimpleIndexChromaPipeline
from langchain_core.documents import Document
import tempfile
import os

router = APIRouter(prefix="/chroma", tags=["Chroma Index Operations"])

# Original Indexer Endpoints

@router.post("/{collection_name}/add_documents")
async def add_documents(collection_name: str, documents: List[dict] = Body(...)):
    try:
        indexer = ChromaIndexer(collection_name)
        docs = [Document(**doc) for doc in documents]
        indexer.add_documents(docs)
        return {"message": f"{len(docs)} documents added to collection '{collection_name}'"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{collection_name}/search")
async def search_documents(collection_name: str, query: str, k: int = 4):
    try:
        indexer = ChromaIndexer(collection_name)
        results = indexer.similarity_search(query, k)
        return {"results": [doc.dict() for doc in results]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/{collection_name}/documents/{document_id}")
async def delete_document(collection_name: str, document_id: str):
    try:
        indexer = ChromaIndexer(collection_name)
        indexer.delete_document(document_id)
        return {"message": f"Document '{document_id}' deleted from collection '{collection_name}'"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.put("/{collection_name}/documents/{document_id}")
async def update_document(collection_name: str, document_id: str, document: dict = Body(...)):
    try:
        indexer = ChromaIndexer(collection_name)
        doc = Document(**document)
        indexer.update_document(document_id, doc)
        return {"message": f"Document '{document_id}' updated in collection '{collection_name}'"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{collection_name}/count")
async def count_documents(collection_name: str):
    try:
        indexer = ChromaIndexer(collection_name)
        count = indexer.count_documents()
        return {"count": count}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Pipeline Processing Endpoints

@router.post("/{collection_name}/process_pdfs")
async def process_pdfs(collection_name: str, files: List[UploadFile] = File(...)):
    try:
        pipeline = SimpleIndexChromaPipeline(collection_name)
        processed_docs = []
        
        with tempfile.TemporaryDirectory() as temp_dir:
            for file in files:
                temp_file_path = os.path.join(temp_dir, file.filename)
                with open(temp_file_path, "wb") as buffer:
                    buffer.write(await file.read())
                processed_docs.extend(pipeline.process_pdf(temp_file_path))
        
        return {"message": f"{len(processed_docs)} documents processed and added to collection '{collection_name}'"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/{collection_name}/process_folder")
async def process_folder(collection_name: str, folder_path: str = Body(..., embed=True)):
    try:
        pipeline = SimpleIndexChromaPipeline(collection_name)
        processed_docs = pipeline.process_folder(folder_path)
        return {"message": f"{len(processed_docs)} documents processed and added to collection '{collection_name}'"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
========================================

File Name: api\routers\download_router.py
========================================
from fastapi import APIRouter, Request, HTTPException
from fastapi.responses import RedirectResponse
from app.core.loaders.gdrive_loader import GDriveLoader

router = APIRouter(prefix="/gdrive", tags=["Google Drive"])

gdrive_loader = GDriveLoader()

@router.get("/authorize")
async def authorize():
    authorization_url, _ = gdrive_loader.authenticate()
    return RedirectResponse(url=authorization_url)

@router.get("/oauth2callback")
async def oauth2callback(request: Request):
    try:
        state = request.query_params.get('state')
        authorization_response = str(request.url)
        gdrive_loader.set_credentials(authorization_response, state)
        # Redirect to the Streamlit UI with a success parameter
        return RedirectResponse(url="http://localhost:8501/Chroma_Index_Operations?auth_success=true")
    except Exception as error:
        raise HTTPException(status_code=500, detail=str(error))

@router.get("/download_files/{folder_id}")
async def download_files(folder_id: str):
    try:
        downloaded_files = gdrive_loader.download_files(folder_id)
        if not downloaded_files:
            raise HTTPException(status_code=404, detail=f"No files found in folder ID: {folder_id}")
        return {"message": f"Files downloaded successfully!", "files": downloaded_files}
    except Exception as error:
        raise HTTPException(status_code=500, detail=str(error))
========================================

File Name: api\routers\__init__.py
========================================

========================================

File Name: core\__init__.py
========================================

========================================

File Name: core\agents\__init__.py
========================================

========================================

File Name: core\agents\langgraph\__init__.py
========================================

========================================

File Name: core\agents\langgraph\complex_agent\agent.py
========================================
from .graph import rag_pipeline
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

class ComplexRAGAgent:
    def __init__(self):
        self.pipeline = rag_pipeline

    def run(self, question: str):
        inputs = {"question": question}
        result = self.pipeline.invoke(inputs)
        return result["generation"]

    def stream(self, question: str):
        inputs = {"question": question}
        for output in self.pipeline.stream(inputs, stream_mode='updates'):
            yield output
               
========================================

File Name: core\agents\langgraph\complex_agent\chains.py
========================================
from typing import Literal
import os
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from .prompts import(
    rag_prompt,
    db_query_rewrite_prompt,
    hallucination_prompt,
    answer_prompt,
    query_feedback_prompt,
    generation_feedback_prompt,
    give_up_prompt,
    grade_doc_prompt,
    knowledge_extraction_prompt,
    router_prompt,
    websearch_query_rewrite_prompt,
    simple_question_prompt
)
from dotenv import load_dotenv

load_dotenv()

class GradeHallucinations(BaseModel):
    binary_score: Literal["yes", "no"] = Field(
        description="Answer is grounded in the facts, 'yes' or 'no'"
    )

class GradeDocuments(BaseModel):
    binary_score: Literal["yes", "no"] = Field(
        description="Document is relevant to the question, 'yes' or 'no'"
    )

class GradeAnswer(BaseModel):
    binary_score: Literal["yes", "no"] = Field(
        description="Answer addresses the question, 'yes' or 'no'"
    )

class RouteQuery(BaseModel):
    route: Literal["vectorstore", "websearch", "QA_LM"] = Field(
        description="Given a user question choose to route it to web search (websearch), a vectorstore (vectorstore), or a QA language model (QA_LM).",
    )

llm_engine = ChatOpenAI(model='gpt-4o-mini')

rag_chain = rag_prompt | llm_engine | StrOutputParser()
db_query_rewriter = db_query_rewrite_prompt | llm_engine | StrOutputParser()
hallucination_grader = hallucination_prompt | llm_engine.with_structured_output(GradeHallucinations)
answer_grader = answer_prompt | llm_engine.with_structured_output(GradeAnswer)
query_feedback_chain = query_feedback_prompt | llm_engine | StrOutputParser()
generation_feedback_chain = generation_feedback_prompt | llm_engine | StrOutputParser()
give_up_chain = give_up_prompt | llm_engine | StrOutputParser()
retrieval_grader = grade_doc_prompt | llm_engine.with_structured_output(GradeDocuments)
knowledge_extractor = knowledge_extraction_prompt | llm_engine | StrOutputParser()
question_router = router_prompt | llm_engine.with_structured_output(RouteQuery)
websearch_query_rewriter = websearch_query_rewrite_prompt | llm_engine | StrOutputParser()
simple_question_chain = simple_question_prompt | llm_engine | StrOutputParser()
========================================

File Name: core\agents\langgraph\complex_agent\graph.py
========================================
from langgraph.graph import END, StateGraph, START
from .nodes import (
    GraphState,
    db_query_rewriting_node,
    retriever_node,
    generation_node,
    router_node,
    query_feedback_node,
    generation_feedback_node,
    simple_question_node,
    web_search_node,
    websearch_query_rewriting_node,
    give_up_node,
    filter_relevant_documents_node,
    knowledge_extractor_node,
    answer_evaluation_node,
    search_mode_node,
    relevant_documents_validation_node
)


pipeline = StateGraph(GraphState)

pipeline.add_node('db_query_rewrite_node', db_query_rewriting_node)
pipeline.add_node('retrieval_node', retriever_node)
pipeline.add_node('generator_node', generation_node)
pipeline.add_node('query_feedback_node', query_feedback_node)
pipeline.add_node('generation_feedback_node', generation_feedback_node)
pipeline.add_node('simple_question_node', simple_question_node)
pipeline.add_node('websearch_query_rewriting_node', websearch_query_rewriting_node)
pipeline.add_node('web_search_node', web_search_node)
pipeline.add_node('give_up_node', give_up_node)
pipeline.add_node('filter_docs_node', filter_relevant_documents_node)
pipeline.add_node('extract_knowledge_node', knowledge_extractor_node)

pipeline.add_conditional_edges(
    START, 
    router_node,
    {
        "vectorstore": 'db_query_rewrite_node',
        "websearch": 'websearch_query_rewriting_node',
        "QA_LM": 'simple_question_node'
    },
)

pipeline.add_edge('db_query_rewrite_node', 'retrieval_node')
pipeline.add_edge('retrieval_node', 'filter_docs_node')
pipeline.add_edge('extract_knowledge_node', 'generator_node')
pipeline.add_edge('websearch_query_rewriting_node', 'web_search_node')
pipeline.add_edge('web_search_node', 'filter_docs_node')
pipeline.add_edge('generation_feedback_node', 'generator_node')
pipeline.add_edge('simple_question_node', END)
pipeline.add_edge('give_up_node', END)

pipeline.add_conditional_edges(
    'generator_node', 
    answer_evaluation_node,
    {
        "useful": END,
        "not relevant": 'query_feedback_node',
        "hallucination": 'generation_feedback_node',
        "max_generation_reached": 'give_up_node'
    }  
)

pipeline.add_conditional_edges(
    'query_feedback_node', 
    search_mode_node,
    {
        "vectorstore": 'db_query_rewrite_node',
        "websearch": 'websearch_query_rewriting_node',
    }
)

pipeline.add_conditional_edges(
    'filter_docs_node', 
    relevant_documents_validation_node,
    {
        "knowledge_extraction": 'extract_knowledge_node',
        "websearch": 'websearch_query_rewriting_node',
        "vectorstore": 'db_query_rewrite_node',
        "max_db_search": 'websearch_query_rewriting_node',
        "max_websearch": 'give_up_node'
    }
)

rag_pipeline = pipeline.compile()
========================================

File Name: core\agents\langgraph\complex_agent\nodes.py
========================================
from app.core.indexers.chroma_indexer import ChromaIndexer
from .chains import (
    rag_chain,
    db_query_rewriter,
    hallucination_grader,
    answer_grader,
    generation_feedback_chain,
    query_feedback_chain,
    retrieval_grader,
    knowledge_extractor,
    question_router,
    simple_question_chain,
    give_up_chain,
    websearch_query_rewriter
)
from .tools import web_search_tool
from .state import GraphState
import logging

logger = logging.getLogger(__name__)

indexer = ChromaIndexer("default_collection")
retriever = indexer.as_retriever()

MAX_RETRIEVALS = 3
MAX_GENERATIONS = 3


def retriever_node(state: GraphState):
    new_documents = retriever.invoke(state.rewritten_question)
    new_documents = [d.page_content for d in new_documents]
    state.documents.extend(new_documents)
    return {
        "documents": state.documents, 
        "retrieval_num": state.retrieval_num + 1
    }

def generation_node(state: GraphState):
    generation = rag_chain.invoke({
        "context": "\n\n".join(state.documents), 
        "question": state.question, 
        "feedback": "\n".join(state.generation_feedbacks)
    })
    return {
        "generation": generation,
        "generation_num": state.generation_num + 1
    }

def db_query_rewriting_node(state: GraphState):
    rewritten_question = db_query_rewriter.invoke({
        "question": state.question,
        "feedback": "\n".join(state.query_feedbacks)
    })
    return {"rewritten_question": rewritten_question, "search_mode": "vectorstore"} 

def answer_evaluation_node(state: GraphState):
    # assess hallucination
    hallucination_grade = hallucination_grader.invoke(
        {"documents": state.documents, "generation": state.generation}
    )
    if hallucination_grade.binary_score == "yes":
        # if no hallucination, assess relevance
        answer_grade = answer_grader.invoke({
            "question": state.question, 
            "generation": state.generation
        })
        if answer_grade.binary_score == "yes":
            # no hallucination and relevant
            return "useful"
        elif state.generation_num > MAX_GENERATIONS:
            return "max_generation_reached"
        else:
            # no hallucination but not relevant
            return "not relevant"
    elif state.generation_num > MAX_GENERATIONS:
        return "max_generation_reached"
    else:
        # we have hallucination
        return "hallucination" 
    
def generation_feedback_node(state: GraphState):
    feedback = generation_feedback_chain.invoke({
        "question": state.question,
        "documents": "\n\n".join(state.documents),
        "generation": state.generation
    })

    feedback = 'Feedback about the answer "{}": {}'.format(
        state.generation, feedback
    )
    state.generation_feedbacks.append(feedback)
    return {"generation_feedbacks": state.generation_feedbacks}

def query_feedback_node(state: GraphState):
    feedback = query_feedback_chain.invoke({
        "question": state.question,
        "rewritten_question": state.rewritten_question,
        "documents": "\n\n".join(state.documents),
        "generation": state.generation
    })

    feedback = 'Feedback about the query "{}": {}'.format(
        state.rewritten_question, feedback
    )
    state.query_feedbacks.append(feedback)
    return {"query_feedbacks": state.query_feedbacks}

def give_up_node(state: GraphState):
    response = give_up_chain.invoke(state.question)
    return {"generation": response}

def filter_relevant_documents_node(state: GraphState):
    # first, we grade every documents
    grades = retrieval_grader.batch([
        {"question": state.question, "document": doc} 
        for doc in state.documents
    ])
    # Then we keep only the documents that were graded as relevant
    filtered_docs = [
        doc for grade, doc 
        in zip(grades, state.documents) 
        if grade.binary_score == 'yes'
    ]

    # If we didn't get any relevant document, let's capture that 
    # as a feedback for the next retrieval iteration
    if not filtered_docs:
        feedback = 'Feedback about the query "{}": did not generate any relevant documents.'.format(
            state.rewritten_question
        )
        state.query_feedbacks.append(feedback)

    return {
        "documents": filtered_docs, 
        "query_feedbacks": state.query_feedbacks
    }

def knowledge_extractor_node(state: GraphState):
    filtered_docs = knowledge_extractor.batch([
        {"question": state.question, "document": doc} 
        for doc in state.documents
    ])
    # we keep only the non empty documents
    filtered_docs = [doc for doc in filtered_docs if doc]
    return {"documents": filtered_docs}

def router_node(state: GraphState):
    route_query = question_router.invoke(state.question)
    return route_query.route

def simple_question_node(state: GraphState):
    answer = simple_question_chain.invoke(state.question)
    return {"generation": answer, "search_mode": "QA_LM"}

def websearch_query_rewriting_node(state: GraphState):
    rewritten_question = websearch_query_rewriter.invoke({
        "question": state.question, 
        "feedback": "\n".join(state.query_feedbacks)
    })
    if state.search_mode != "websearch":
        state.retrieval_num = 0    
    return {
        "rewritten_question": rewritten_question, 
        "search_mode": "websearch",
        "retrieval_num": state.retrieval_num
    }

def web_search_node(state: GraphState):
    try:
        new_docs = web_search_tool.invoke(
            {"query": state.rewritten_question}
        )
        
        if isinstance(new_docs, str):
            web_results = [new_docs]
        elif isinstance(new_docs, list):
            web_results = [d.get("content", str(d)) if isinstance(d, dict) else str(d) for d in new_docs]
        else:
            web_results = [str(new_docs)]
        
        state.documents.extend(web_results)
        return {
            "documents": state.documents, 
            "retrieval_num": state.retrieval_num + 1
        }
    except Exception as e:
        return {
            "error": f"Web search failed: {str(e)}",
            "retrieval_num": state.retrieval_num + 1
        }

def search_mode_node(state: GraphState):
    return state.search_mode

def relevant_documents_validation_node(state: GraphState):
    if state.documents:
        # we have relevant documents
        return "knowledge_extraction"
    elif state.search_mode == 'vectorsearch' and state.retrieval_num > MAX_RETRIEVALS:
        # we don't have relevant documents
        # and we reached the maximum number of retrievals
        return "max_db_search"
    elif state.search_mode == 'websearch' and state.retrieval_num > MAX_RETRIEVALS:
        # we don't have relevant documents
        # and we reached the maximum number of websearches
        return "max_websearch"
    else:
        # we don't have relevant documents
        # so we retry the search
        return state.search_mode
========================================

File Name: core\agents\langgraph\complex_agent\prompts.py
========================================
from langchain_core.prompts import ChatPromptTemplate


system_prompt = """
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Additional feedback may be provided about a previous version of the answer. Make sure to utilize that feedback to improve the answer.
Only provide the answer and nothing else!
"""

human_prompt = """
Question: {question}

Context: 
{context}

Here is the feedback about previous versions of the answer:
{feedback}

Answer:
"""

rag_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

#***#
system_prompt = """
You a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval.
The vectorstore contains information about AI papers. Look at the input and try to reason about the underlying semantic intent / meaning.
Additional feedback may be provided for why a previous version of the question didn't lead to a valid response. Make sure to utilize that feedback to generate a better question.
Only respond with the rewritten question and nothing else! 
"""

human_prompt = """
Here is the initial question: {question}

Here is the feedback about previous versions of the question:
{feedback}

Formulate an improved question.
Rewritten question:
"""

db_query_rewrite_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.
Give a binary score 'yes' or 'no'. 'yes' means that the answer is grounded in / supported by the set of facts.
"""

human_prompt = """
Set of facts:

{documents}

LLM generation: {generation}
"""

hallucination_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
) 

system_prompt = """
You are a grader assessing whether an answer addresses / resolves a question.
Give a binary score 'yes' or 'no'. 'yes' means that the answer resolves the question.
"""

human_prompt = """
User question: {question} 

LLM generation: {generation}
"""


answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
Your role is to give feedback on a the LLM generated answer. The LLM generation is NOT grounded in the set of retrieved facts.
Explain how the generated answer could be improved so that it is only solely grounded in the retrieved facts.  
Only provide your feedback and nothing else!
"""

human_prompt = """
User question: {question}

Retrieved facts: 
{documents}

Wrong generated answer: {generation}
"""

generation_feedback_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
Your role is to give feedback on a the text query used to retrieve documents. Those retrieved documents are used as context to answer a user question.
The following generated answer doesn't address the question! Explain how the query could be improved so that the retrieved documents could be more relevant to the question. 
Only provide your feedback and nothing else!
"""

human_prompt = """
User question: {question}

Text query: {rewritten_question}

Retrieved documents: 
{documents}

Wrong generated answer: {generation}
"""

query_feedback_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
You job is to generate an apology for not being able to provide a correct answer to a user question.
The question were used to retrieve documents from a database and a websearch and none of them were able to provide enough context to answer the user question.
Explain to the user that you couldn't answer the question.
"""

give_up_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "User question: {question} \n\n Answer:"),
    ]
)

system_prompt = """
You are a grader assessing relevance of a retrieved document to a user question. 
It does not need to be a stringent test. The goal is to filter out erroneous retrievals.
If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant.
Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. 'yes' means that the document contains relevant information.
"""

human_prompt = """
Retrieved document: {document}

User question: {question}
"""

grade_doc_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
You are a knowledge refinement engine. Your job is to extract the information from a document that could be relevant to a user question. 
The goal is to filter out the noise and keep only the information that can provide context to answer the user question.
If the document contains keyword(s) or semantic meaning related to the user question, consider it as relevant.
DO NOT modify the text, only return the original text that is relevant to the user question. 
"""

human_prompt = """
Retrieved document: {document}

User question: {question}
"""

knowledge_extraction_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

#***#
system_prompt = """
You are an expert at routing a user question to a vectorstore, a websearch or a simple QA language model.
The vectorstore contains documents related to AI papers.
If you can answer the question without any additional context or if a websearch could not provide additional context, route it to the QA language model.
If you need additional context and it is a question about AI papers, use the vectorstore, otherwise, use websearch.
"""

router_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{question}"),
    ]
)

system_prompt = """
You are a question re-writer that converts an input question to a better version that is optimized for web search. 
Look at the input and try to reason about the underlying semantic intent / meaning.
Additional feedback may be provided for why a previous version of the question didn't lead to a valid response. Make sure to utilize that feedback to generate a better question.
Only respond with the rewritten question and nothing else! 
"""

human_prompt = """
Here is the initial question: {question}

Here is the feedback about previous versions of the question:
{feedback}

Formulate an improved question.
Rewritten question:
"""

websearch_query_rewrite_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)

system_prompt = """
You are a helpful assistant. Provide a answer to the user.
"""

simple_question_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{question}"),
    ]
)
========================================

File Name: core\agents\langgraph\complex_agent\state.py
========================================
from pydantic import BaseModel
from typing import List, Literal, Optional

class GraphState(BaseModel):

    question: Optional[str] = None
    generation: Optional[str] = None
    documents: List[str] = []
    rewritten_question: Optional[str] = None
    query_feedbacks: List[str] = []
    generation_feedbacks: List[str] = []
    generation_num: int = 0
    retrieval_num: int = 0
    search_mode: Literal["vectorstore", "websearch", "QA_LM"] = "QA_LM"
========================================

File Name: core\agents\langgraph\complex_agent\tools.py
========================================
import os
from langchain_community.tools.tavily_search import TavilySearchResults
from dotenv import load_dotenv

load_dotenv()

web_search_tool = TavilySearchResults(k=3)
========================================

File Name: core\agents\langgraph\complex_agent\__init__.py
========================================

========================================

File Name: core\agents\langgraph\simple_agent\agent.py
========================================
from langchain_core.retrievers import BaseRetriever
from langchain_core.runnables import RunnableSequence
from .graph import build_rag_pipeline
from .state import GraphState

class SimpleRAGAgent:
    def __init__(self, retriever: BaseRetriever, generation_chain: RunnableSequence):
        self.retriever = retriever
        self.generation_chain = generation_chain
        self.pipeline = build_rag_pipeline(self.retriever, self.generation_chain)

    def run(self, question: str):
        inputs = {"question": question}
        result = self.pipeline.invoke(inputs)
        return result["generation"]

    def stream(self, question: str):
        inputs = {"question": question}
        for output in self.pipeline.stream(inputs, stream_mode='updates'):
            yield output
               
            
========================================

File Name: core\agents\langgraph\simple_agent\chains.py
========================================
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from .prompts import rag_prompt

load_dotenv()

llm_engine = ChatOpenAI(model='gpt-4o-mini')  
rag_chain = rag_prompt | llm_engine | StrOutputParser()
========================================

File Name: core\agents\langgraph\simple_agent\graph.py
========================================
from langgraph.graph import StateGraph, START, END
from langchain_core.retrievers import BaseRetriever
from langchain_core.runnables import RunnableSequence
from .state import GraphState
from .nodes import create_retriever_node, create_generation_node

def build_rag_pipeline(retriever: BaseRetriever, generation_chain: RunnableSequence):
    pipeline = StateGraph(GraphState)
    
    # Create nodes
    retriever_node = create_retriever_node(retriever)
    generator_node = create_generation_node(generation_chain)
    
    # Add nodes
    pipeline.add_node('retrieval_node', retriever_node)
    pipeline.add_node('generator_node', generator_node)
    
    # Connect nodes
    pipeline.add_edge(START, 'retrieval_node')
    pipeline.add_edge('retrieval_node', 'generator_node')
    pipeline.add_edge('generator_node', END)
    
    return pipeline.compile()
========================================

File Name: core\agents\langgraph\simple_agent\nodes.py
========================================
from langchain_core.retrievers import BaseRetriever
from langchain_core.runnables import RunnableSequence
from app.core.indexers.chroma_indexer import ChromaIndexer
from .chains import rag_chain
from .state import GraphState

def create_retriever_node(retriever: BaseRetriever):
    def retriever_node(state: GraphState):
        new_documents = retriever.invoke(state.question)
        new_documents = [d.page_content for d in new_documents]
        state.documents.extend(new_documents)
        return {"documents": state.documents}
    return retriever_node

def create_generation_node(generation_chain: RunnableSequence):
    def generation_node(state: GraphState):
        generation = generation_chain.invoke({
            "context": "\n\n".join(state.documents), 
            "question": state.question, 
        })
        return {"generation": generation}
    return generation_node
========================================

File Name: core\agents\langgraph\simple_agent\prompts.py
========================================
from langchain_core.prompts import ChatPromptTemplate

system_prompt = """
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
Only provide the answer and nothing else!
"""

human_prompt = """
Question: {question}

Context: 
{context}

Answer:
"""

rag_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", human_prompt),
    ]
)


========================================

File Name: core\agents\langgraph\simple_agent\state.py
========================================
from typing import List, Optional
from pydantic import BaseModel

class GraphState(BaseModel):
    question: Optional[str] = None
    generation: Optional[str] = None
    documents: List[str] = []
========================================

File Name: core\agents\langgraph\simple_agent\__init__.py
========================================
from .agent import SimpleRAGAgent
from app.core.indexers.chroma_indexer import ChromaIndexer
from .chains import rag_chain

def create_chroma_simple_rag_agent(collection_name: str = "default_collection"):
    indexer = ChromaIndexer(collection_name)
    retriever = indexer.as_retriever()
    return SimpleRAGAgent(retriever, rag_chain)
========================================

File Name: core\chunkers\simple_chunker.py
========================================
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

class SimpleChunker:
    def __init__(self, chunk_size=10000, chunk_overlap=200):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        return self.text_splitter.split_documents(documents)
    
    def split_text(self, text: str) -> List[str]:
        return self.text_splitter.split_text(text)
========================================

File Name: core\chunkers\__init__.py
========================================

========================================

File Name: core\indexers\chroma_indexer.py
========================================
from typing import List
from langchain_core.documents import Document
from app.databases.chroma_db import chroma_db

class ChromaIndexer:
    def __init__(self, collection_name="default_collection"):
        self.collection_name = collection_name
        self.vectorstore = chroma_db.initialize_db(collection_name)
    
    def add_documents(self, documents: List[Document]):
        self.vectorstore.add_documents(documents)
        #self.vectorstore.persist()
    
    def similarity_search(self, query: str, k: int = 4):
        return self.vectorstore.similarity_search(query, k=k)
    
    def update_document(self, document_id: str, document: Document):
        self.vectorstore.update_document(document_id, document)
        #self.vectorstore.persist()
    
    def delete_document(self, document_id: str):
        self.vectorstore.delete([document_id])
        #self.vectorstore.persist()
    
    def as_retriever(self, search_kwargs=None):
        if search_kwargs is None:
            search_kwargs = {"k": 4}
        return self.vectorstore.as_retriever(search_kwargs=search_kwargs)
    
    def get_collection(self):
        return chroma_db.load_db(self.collection_name)
    
    def count_documents(self):
        return self.vectorstore._collection.count()
========================================

File Name: core\indexers\__init__.py
========================================

========================================

File Name: core\loaders\gdrive_loader.py
========================================
import os
from app.services import google_drive
import logging

logger = logging.getLogger(__name__)

class GDriveLoader:
    def __init__(self):
        self.service = None

    def authenticate(self):
        return google_drive.authenticate()

    def set_credentials(self, authorization_response, state):
        creds = google_drive.get_credentials_from_callback(authorization_response, state)
        google_drive.save_credentials(creds)

    def initialize_service(self):
        if not self.service:
            self.service = google_drive.get_service()

    def download_files(self, folder_id):
        """Download files from a Google Drive folder"""
        try:
            if not self.service:
                self.initialize_service()

            downloaded_files = google_drive.download_files(self.service, folder_id)
            
            if not downloaded_files:
                logger.warning(f"No files found in folder {folder_id}")
                return []

            # Return only the filenames for the UI
            return [os.path.basename(file_path) for file_path in downloaded_files]

        except Exception as e:
            logger.error(f"Error downloading files: {str(e)}")
            raise
========================================

File Name: core\loaders\__init__.py
========================================

========================================

File Name: core\pipes\simple_index_pipeline.py
========================================
import os
from typing import List
from langchain_community.document_loaders import PyPDFLoader
from app.core.chunkers.simple_chunker import SimpleChunker
from app.core.indexers.chroma_indexer import ChromaIndexer
from langchain_core.documents import Document

class SimpleIndexChromaPipeline:
    def __init__(self, collection_name: str, chunk_size: int = 10000, chunk_overlap: int = 200):
        self.collection_name = collection_name
        self.loader = PyPDFLoader
        self.chunker = SimpleChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        self.indexer = ChromaIndexer(collection_name=collection_name)

    def process_pdf(self, file_path: str) -> List[Document]:
        # Load PDF
        loader = self.loader(file_path)
        documents = loader.load()
        
        # Add file metadata to each document
        filename = os.path.basename(file_path)
        for doc in documents:
            doc.metadata.update({
                "source_file": filename,
                "file_path": file_path,
                "page_number": doc.metadata.get("page", 1)
            })

        # Chunk documents
        chunked_documents = self.chunker.split_documents(documents)
        
        # Ensure metadata is preserved in chunks
        for chunk in chunked_documents:
            if not chunk.metadata.get("source_file"):
                chunk.metadata.update({
                    "source_file": filename,
                    "file_path": file_path,
                    "page_number": chunk.metadata.get("page", 1)
                })

        # Index documents
        self.indexer.add_documents(chunked_documents)

        return chunked_documents

    def process_multiple_pdfs(self, file_paths: List[str]) -> List[Document]:
        all_documents = []
        for file_path in file_paths:
            all_documents.extend(self.process_pdf(file_path))
        return all_documents

    def process_folder(self, folder_path: str) -> List[Document]:
        all_documents = []
        for root, _, files in os.walk(folder_path):
            for file in files:
                if file.lower().endswith('.pdf'):
                    file_path = os.path.join(root, file)
                    all_documents.extend(self.process_pdf(file_path))
        return all_documents
========================================

File Name: core\pipes\__init__.py
========================================

========================================

File Name: core\processors\__init__.py
========================================

========================================

File Name: databases\chroma_db.py
========================================
import os
from chromadb import PersistentClient
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
import logging

load_dotenv()
logger = logging.getLogger(__name__)

class ChromaDB:
    def __init__(self, persist_directory="./app/databases/chroma_db", embedding_function=OpenAIEmbeddings()):
        self.persist_directory = persist_directory
        self.embedding_function = embedding_function
        self.client = None
        self.vectorstore = None
        
    def _connect(self):
        """Create or connect to the ChromaDB client"""
        try:
            if not self.client:
                if not os.path.exists(self.persist_directory):
                    os.makedirs(self.persist_directory)
                self.client = PersistentClient(path=self.persist_directory)
        except Exception as e:
            logger.error(f"Error connecting to ChromaDB: {str(e)}")
            raise
    
    def initialize_db(self, collection_name="default_collection"):
        """Initialize or connect to a ChromaDB collection"""
        try:
            self._connect()
            
            self.vectorstore = Chroma(
                collection_name=collection_name,
                embedding_function=self.embedding_function,
                persist_directory=self.persist_directory,
                client=self.client
            )
            return self.vectorstore
        except Exception as e:
            logger.error(f"Error initializing database: {str(e)}")
            raise
    
    def create_collection(self, collection_name):
        """Create a new collection"""
        try:
            self._connect()
            return self.initialize_db(collection_name)
        except Exception as e:
            logger.error(f"Error creating collection: {str(e)}")
            raise
    
    def delete_collection(self, collection_name):
        """Delete a collection"""
        try:
            self._connect()
            
            # Clear the vectorstore if it's the collection being deleted
            if self.vectorstore and self.vectorstore._collection.name == collection_name:
                self.vectorstore = None
            
            # Delete the collection
            self.client.delete_collection(collection_name)
            
        except Exception as e:
            logger.error(f"Error deleting collection: {str(e)}")
            raise
    
    def list_collections(self):
        """List all collections"""
        try:
            self._connect()
            return [col.name for col in self.client.list_collections()]
        except Exception as e:
            logger.error(f"Error listing collections: {str(e)}")
            raise

# Global instance
chroma_db = ChromaDB()
========================================

File Name: databases\__init__.py
========================================

========================================

File Name: notebooks\__init__.py
========================================

========================================

File Name: services\google_drive.py
========================================
import os
import io
import pickle
from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from google.auth.transport.requests import Request as GoogleRequest
from googleapiclient.errors import HttpError
import logging

# Allow OAuth2 insecure transport for development
os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'

logger = logging.getLogger(__name__)

SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
CLIENT_SECRETS_FILE = 'app/configs/credentials.json'
REDIRECT_URI = "http://localhost:8000/gdrive/oauth2callback"
DOWNLOAD_FOLDER = 'data/raw_data'

# MIME type mappings for Google Workspace files
GOOGLE_MIME_TYPES = {
    'application/vnd.google-apps.document': ('application/pdf', '.pdf'),
    'application/vnd.google-apps.spreadsheet': ('application/pdf', '.pdf'),
    'application/vnd.google-apps.presentation': ('application/pdf', '.pdf'),
    'application/vnd.google-apps.drawing': ('application/pdf', '.pdf'),
}

def authenticate():
    flow = Flow.from_client_secrets_file(CLIENT_SECRETS_FILE, scopes=SCOPES)
    flow.redirect_uri = REDIRECT_URI
    authorization_url, state = flow.authorization_url(access_type='offline', include_granted_scopes='true')
    return authorization_url, state

def get_credentials_from_callback(authorization_response, state):
    flow = Flow.from_client_secrets_file(CLIENT_SECRETS_FILE, scopes=SCOPES, state=state)
    flow.redirect_uri = REDIRECT_URI
    flow.fetch_token(authorization_response=authorization_response)
    return flow.credentials

def save_credentials(creds):
    with open('token.pickle', 'wb') as token:
        pickle.dump(creds, token)

def get_service():
    creds = None
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(GoogleRequest())
        else:
            raise Exception("Credentials are not valid, please authorize again.")
    return build('drive', 'v3', credentials=creds)

def list_files_in_folder(service, folder_id):
    query = f"'{folder_id}' in parents and (mimeType='application/pdf' or mimeType contains 'application/vnd.google-apps.')"
    try:
        results = service.files().list(
            q=query,
            pageSize=1000,
            fields="nextPageToken, files(id, name, mimeType)"
        ).execute()
        return results.get('files', [])
    except HttpError as error:
        logger.error(f"Error listing files: {str(error)}")
        raise

def download_file(service, file_id, file_name, mime_type, folder_path=DOWNLOAD_FOLDER):
    """Download or export a file from Google Drive"""
    try:
        # Create the download folder if it doesn't exist
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)

        if mime_type in GOOGLE_MIME_TYPES:
            # Export Google Workspace files
            export_mime_type, extension = GOOGLE_MIME_TYPES[mime_type]
            request = service.files().export_media(
                fileId=file_id,
                mimeType=export_mime_type
            )
            # Ensure the filename has the correct extension
            if not file_name.endswith(extension):
                file_name = f"{file_name}{extension}"
        else:
            # Direct download for other files
            request = service.files().get_media(fileId=file_id)

        file_path = os.path.join(folder_path, file_name)
        fh = io.FileIO(file_path, 'wb')
        downloader = MediaIoBaseDownload(fh, request)

        done = False
        while not done:
            status, done = downloader.next_chunk()
            logger.info(f"Downloading {file_name}: {int(status.progress() * 100)}%")

        logger.info(f"Downloaded {file_name} to {file_path}")
        return file_path

    except Exception as e:
        logger.error(f"Error downloading file {file_name}: {str(e)}")
        raise

def download_files(service, folder_id):
    """Download all files from a folder"""
    try:
        files = list_files_in_folder(service, folder_id)
        if not files:
            return []

        downloaded_files = []
        for file in files:
            try:
                file_path = download_file(
                    service,
                    file['id'],
                    file['name'],
                    file['mimeType']
                )
                downloaded_files.append(file_path)
            except Exception as e:
                logger.error(f"Error downloading {file['name']}: {str(e)}")
                continue

        return downloaded_files

    except Exception as e:
        logger.error(f"Error downloading files from folder: {str(e)}")
        raise
========================================

File Name: services\__init__.py
========================================

========================================

File Name: utils\__init__.py
========================================

========================================

